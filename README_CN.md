<p align="left">
   <a href="README.md">English</a>  ï½œ ä¸­æ–‡</a>&nbsp
</p>
<br><br>

<p align="center">
 <img src="imgs/hunyuanlogo.png" width="400"/> <br>
</p><p></p>


<p align="center">
    ğŸ¤—&nbsp;<a href="https://huggingface.co/collections/tencent/hy-mt15"><b>Hugging Face</b></a>&nbsp;&nbsp;|&nbsp;&nbsp;
    <img src="https://avatars.githubusercontent.com/u/109945100?s=200&v=4" width="16"/>&nbsp;<a href="https://modelscope.cn/collections/Tencent-Hunyuan/HY-MT15"><b>ModelScope</b></a>&nbsp;&nbsp;|&nbsp;&nbsp;
</p>

<p align="center">
    ğŸ–¥ï¸&nbsp;<a href="https://hunyuan.tencent.com" style="color: red;"><b>Official Website</b></a>&nbsp;&nbsp;|&nbsp;&nbsp;
    ğŸ•¹ï¸&nbsp;<a href="https://hunyuan.tencent.com/chat/HunyuanDefault?from=modelSquare&modelId=hunyuan-mt-1.8b"><b>Demo</b></a>&nbsp;&nbsp;&nbsp;&nbsp;
</p>

<p align="center">
    <a href="https://github.com/Tencent-Hunyuan/HY-MT"><b>GITHUB</b></a>
</p>


## æ¨¡å‹ä»‹ç»

æ··å…ƒç¿»è¯‘æ¨¡å‹1.5ç‰ˆæœ¬ï¼ŒåŒ…å«ä¸€ä¸ª1.8Bç¿»è¯‘æ¨¡å‹HY-MT1.5-1.8Bå’Œ7Bç¿»è¯‘æ¨¡å‹HY-MT1.5-7Bã€‚ä¸¤ä¸ªæ¨¡å‹å‡é‡ç‚¹æ”¯æŒ33è¯­ç§äº’è¯‘ï¼Œæ”¯æŒ5ç§æ°‘æ±‰/æ–¹è¨€ã€‚å…¶ä¸­HY-MT1.5-7Bæ˜¯æˆ‘ä»¬WMT25å† å†›æ¨¡å‹çš„å‡çº§ç‰ˆï¼Œä¼˜åŒ–è§£é‡Šæ€§ç¿»è¯‘å’Œè¯­ç§æ··æ‚æƒ…å†µï¼Œæ–°å¢æ”¯æŒæœ¯è¯­å¹²é¢„ã€ä¸Šä¸‹æ–‡ç¿»è¯‘ã€å¸¦æ ¼å¼ç¿»è¯‘ã€‚HY-MT1.5-1.8Båœ¨å‚æ•°é‡åªæœ‰ä¸åˆ°HY-MT-7Bçš„ä¸‰åˆ†ä¹‹ä¸€æƒ…å†µä¸‹ï¼Œç¿»è¯‘æ•ˆæœè·ŸHY-MT1.5-7Bç›¸è¿‘ï¼ŒçœŸæ­£åšåˆ°çš„é€Ÿåº¦åˆå¿«æ•ˆæœåˆå¥½ã€‚1.8Bè¿™ä¸ªå°ºå¯¸åœ¨ç»è¿‡é‡åŒ–åï¼Œèƒ½å¤Ÿæ”¯æŒç«¯ä¾§éƒ¨ç½²å’Œå®æ—¶ç¿»è¯‘åœºæ™¯ï¼Œåº”ç”¨é¢å¹¿æ³›ã€‚



### æ ¸å¿ƒç‰¹æ€§ä¸ä¼˜åŠ¿
- HY-MT1.5-1.8BåŒå°ºå¯¸ä¸šç•Œæ•ˆæœæœ€ä¼˜ï¼Œè¶…è¿‡å¤§éƒ¨åˆ†å•†ç”¨ç¿»è¯‘API
- HY-MT1.5-1.8Bæ”¯æŒç«¯ä¾§éƒ¨ç½²å’Œå®æ—¶ç¿»è¯‘åœºæ™¯ï¼Œåº”ç”¨é¢å¹¿æ³›
- HY-MT1.5-7Bç›¸æ¯”9æœˆä»½å¼€æºç‰ˆæœ¬ï¼Œä¼˜åŒ–æ³¨é‡Šå’Œè¯­ç§æ··æ‚æƒ…å†µ
- ä¸¤ä¸ªæ¨¡å‹å‡æ”¯æŒæœ¯è¯­å¹²é¢„ã€ä¸Šä¸‹æ–‡ç¿»è¯‘ã€å¸¦æ ¼å¼ç¿»è¯‘


## æ–°é—»
<br>

* 2025.12.30 æˆ‘ä»¬åœ¨Hugging Faceå¼€æºäº† **HY-MT1.5-1.8B**å’Œ**HY-MT1.5-7B**
* 2025.9.1 æˆ‘ä»¬åœ¨Hugging Faceå¼€æºäº† **Hunyuan-MT-7B**å’Œ**Hunyuan-MT-Chimera-7B**ã€‚

## æ•ˆæœ
<div align='center'>
<img src="imgs/overall_performance.png" width = "80%" />
</div>

æ›´å¤šçš„å®éªŒæ•ˆæœå’Œåˆ†æå¯ä»¥å‚è€ƒæˆ‘ä»¬çš„[æŠ€æœ¯æŠ¥å‘Š](./HY_MT1_5_Technical_Report.pdf)ã€‚

&nbsp;

## æ¨¡å‹é“¾æ¥
| Model Name  | Description | Download |
| ----------- | ----------- |-----------
| HY-MT1.5-1.8B  | æ··å…ƒ1.8Bç¿»è¯‘æ¨¡å‹ |ğŸ¤— [Model](https://huggingface.co/tencent/HY-MT1.5-1.8B)|
| HY-MT1.5-1.8B-FP8 | æ··å…ƒ1.8Bç¿»è¯‘æ¨¡å‹ï¼Œfp8é‡åŒ–    | ğŸ¤— [Model](https://huggingface.co/tencent/HY-MT1.5-1.8B-FP8)|
| HY-MT1.5-7B | æ··å…ƒ7Bç¿»è¯‘æ¨¡å‹    | ğŸ¤— [Model](https://huggingface.co/tencent/HY-MT1.5-7B)|
| HY-MT1.5-7B-FP8 | æ··å…ƒ7Bç¿»è¯‘æ¨¡å‹ï¼Œfp8é‡åŒ–     | ğŸ¤— [Model](https://huggingface.co/tencent/HY-MT1.5-7B-FP8)|


## Prompts

### ä¸­å¤–äº’è¯‘promptæ¨¡æ¿
---
```
å°†ä»¥ä¸‹æ–‡æœ¬ç¿»è¯‘ä¸º{target_language}ï¼Œæ³¨æ„åªéœ€è¦è¾“å‡ºç¿»è¯‘åçš„ç»“æœï¼Œä¸è¦é¢å¤–è§£é‡Šï¼š

{source_text}
```
---

### å¤–å¤–äº’è¯‘promptæ¨¡æ¿
---
```
Translate the following segment into {target_language}, without additional explanation.

{source_text}
```
---


### æœ¯è¯­å¹²é¢„æ¨¡æ¿
---
```
å‚è€ƒä¸‹é¢çš„ç¿»è¯‘ï¼š
{source_term} ç¿»è¯‘æˆ {target_term}

å°†ä»¥ä¸‹æ–‡æœ¬ç¿»è¯‘ä¸º{target_language}ï¼Œæ³¨æ„åªéœ€è¦è¾“å‡ºç¿»è¯‘åçš„ç»“æœï¼Œä¸è¦é¢å¤–è§£é‡Šï¼š
{source_text}
```
---

### ä¸Šä¸‹æ–‡ç¿»è¯‘æ¨¡æ¿
---
```
{context}
å‚è€ƒä¸Šé¢çš„ä¿¡æ¯ï¼ŒæŠŠä¸‹é¢çš„æ–‡æœ¬ç¿»è¯‘æˆ{target_language}ï¼Œæ³¨æ„ä¸éœ€è¦ç¿»è¯‘ä¸Šæ–‡ï¼Œä¹Ÿä¸è¦é¢å¤–è§£é‡Šï¼š
{source_text}

```
---

### å¸¦æ ¼å¼ç¿»è¯‘æ¨¡æ¿
---
```
å°†ä»¥ä¸‹<source></source>ä¹‹é—´çš„æ–‡æœ¬ç¿»è¯‘ä¸ºä¸­æ–‡ï¼Œæ³¨æ„åªéœ€è¦è¾“å‡ºç¿»è¯‘åçš„ç»“æœï¼Œä¸è¦é¢å¤–è§£é‡Šï¼ŒåŸæ–‡ä¸­çš„<sn></sn>æ ‡ç­¾è¡¨ç¤ºæ ‡ç­¾å†…æ–‡æœ¬åŒ…å«æ ¼å¼ä¿¡æ¯ï¼Œéœ€è¦åœ¨è¯‘æ–‡ä¸­ç›¸åº”çš„ä½ç½®å°½é‡ä¿ç•™è¯¥æ ‡ç­¾ã€‚è¾“å‡ºæ ¼å¼ä¸ºï¼š<target>str</target>

<source>{src_text_with_format}</source>
```
---

## ä½¿ç”¨ transformers æ¨ç†
é¦–å…ˆï¼Œéœ€è¦å®‰è£…æœ€æ–°ç‰ˆæœ¬çš„transformersï¼Œæ¨èv4.56.0åŠä»¥ä¸Š
```SHELL
pip install transformers==4.56.0
```

*!!! If you want to load fp8 model with transformers, you need to change the name"ignored_layers" in config.json to "ignore" and upgrade the compressed-tensors to compressed-tensors-0.11.0.*

ä»¥ä¸‹ä»£ç ç‰‡æ®µå±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ transformers åº“åŠ è½½å’Œä½¿ç”¨æ¨¡å‹ã€‚

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import os

model_name_or_path = "tencent/Hunyuan-MT-7B"

tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)
model = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map="auto")  # You may want to use bfloat16 and/or move to GPU here
messages = [
    {"role": "user", "content": "Translate the following segment into Chinese, without additional explanation.\n\nGet something off your chest"},
]
tokenized_chat = tokenizer.apply_chat_template(
    messages,
    tokenize=True
    add_generation_prompt=False,
    return_tensors="pt"
)

outputs = model.generate(tokenized_chat.to(model.device), max_new_tokens=2048)
output_text = tokenizer.decode(outputs[0])
```


æˆ‘ä»¬æ¨èä½¿ç”¨ä¸‹é¢è¿™ç»„å‚æ•°è¿›è¡Œæ¨ç†ã€‚æ³¨æ„ï¼Œæˆ‘ä»¬çš„æ¨¡å‹æ²¡æœ‰é»˜è®¤ system_promptã€‚

```json

{
  "top_k": 20,
  "top_p": 0.6,
  "repetition_penalty": 1.05,
  "temperature": 0.7
}
```

&nbsp;

æ”¯æŒçš„è¯­ç§:
| Languages         | Abbr.   | Chinese Names   |
|-------------------|---------|-----------------|
| Chinese           | zh      | ä¸­æ–‡            |
| English           | en      | è‹±è¯­            |
| French            | fr      | æ³•è¯­            |
| Portuguese        | pt      | è‘¡è„ç‰™è¯­        |
| Spanish           | es      | è¥¿ç­ç‰™è¯­        |
| Japanese          | ja      | æ—¥è¯­            |
| Turkish           | tr      | åœŸè€³å…¶è¯­        |
| Russian           | ru      | ä¿„è¯­            |
| Arabic            | ar      | é˜¿æ‹‰ä¼¯è¯­        |
| Korean            | ko      | éŸ©è¯­            |
| Thai              | th      | æ³°è¯­            |
| Italian           | it      | æ„å¤§åˆ©è¯­        |
| German            | de      | å¾·è¯­            |
| Vietnamese        | vi      | è¶Šå—è¯­          |
| Malay             | ms      | é©¬æ¥è¯­          |
| Indonesian        | id      | å°å°¼è¯­          |
| Filipino          | tl      | è²å¾‹å®¾è¯­        |
| Hindi             | hi      | å°åœ°è¯­          |
| Traditional Chinese | zh-Hant| ç¹ä½“ä¸­æ–‡        |
| Polish            | pl      | æ³¢å…°è¯­          |
| Czech             | cs      | æ·å…‹è¯­          |
| Dutch             | nl      | è·å…°è¯­          |
| Khmer             | km      | é«˜æ£‰è¯­          |
| Burmese           | my      | ç¼…ç”¸è¯­          |
| Persian           | fa      | æ³¢æ–¯è¯­          |
| Gujarati          | gu      | å¤å‰æ‹‰ç‰¹è¯­      |
| Urdu              | ur      | ä¹Œå°”éƒ½è¯­        |
| Telugu            | te      | æ³°å¢å›ºè¯­        |
| Marathi           | mr      | é©¬æ‹‰åœ°è¯­        |
| Hebrew            | he      | å¸Œä¼¯æ¥è¯­        |
| Bengali           | bn      | å­ŸåŠ æ‹‰è¯­        |
| Tamil             | ta      | æ³°ç±³å°”è¯­        |
| Ukrainian         | uk      | ä¹Œå…‹å…°è¯­        |
| Tibetan           | bo      | è—è¯­            |
| Kazakh            | kk      | å“ˆè¨å…‹è¯­        |
| Mongolian         | mn      | è’™å¤è¯­          |
| Uyghur            | ug      | ç»´å¾å°”è¯­        |
| Cantonese         | yue     | ç²¤è¯­            |


## è®­ç»ƒæ•°æ®æ ¼å¼å¤„ç†

å¦‚æœéœ€è¦å¾®è°ƒæˆ‘ä»¬çš„æ¨¡å‹ï¼Œå»ºè®®å°†æ•°æ®å¤„ç†æˆä»¥ä¸‹æ ¼å¼ã€‚

```python
messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "æµ·æ°´ä¸ºä»€ä¹ˆæ˜¯å’¸çš„" },
    {"role": "assistant", "content": "æµ·æ°´æ˜¯å’¸çš„ä¸»è¦æ˜¯å› ä¸ºå…¶ä¸­å«æœ‰è®¸å¤šæº¶è§£åœ¨æ°´ä¸­çš„ç›ç±»å’ŒçŸ¿ç‰©è´¨ã€‚è¿™äº›ç›ç±»å’ŒçŸ¿ç‰©è´¨æ¥è‡ªäºåœ°çƒè¡¨é¢çš„å²©çŸ³å’ŒåœŸå£¤ä¸­çš„åŒ–å­¦ç‰©è´¨ï¼Œéšç€æ—¶é—´çš„æ¨ç§»ï¼Œå®ƒä»¬è¢«å¸¦åˆ°äº†æµ·æ´‹ä¸­ã€‚å½“æµ·æ°´è’¸å‘æ—¶ï¼Œæ°´åˆ†è’¸å‘æ‰äº†ï¼Œä½†ç›ç±»å’ŒçŸ¿ç‰©è´¨ä»ç„¶ç•™åœ¨æ°´ä¸­ï¼Œå¯¼è‡´æµ·æ°´å˜å¾—æ›´åŠ å’¸å‘³ã€‚å› æ­¤ï¼Œæµ·æ°´çš„å’¸åº¦æ˜¯ç”±å…¶ä¸­çš„ç›ç±»å’ŒçŸ¿ç‰©è´¨çš„å«é‡å†³å®šçš„ã€‚"}
]

from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("your_tokenizer_path", trust_remote_code=True)
train_ids = tokenizer.apply_chat_template(messages)
```

&nbsp;

## ä½¿ç”¨ LLaMA-Factory è®­ç»ƒ

æˆ‘ä»¬å°†ä»‹ç»å¦‚ä½•ä½¿ç”¨`LLaMA-Factory`æ¥è¿›è¡Œå¾®è°ƒæ··å…ƒæ¨¡å‹ã€‚

### å®‰è£…ç¯å¢ƒ

å¼€å§‹ä¹‹å‰ï¼Œç¡®ä¿ä½ å·²ç»å®‰è£…äº†ä»¥ä¸‹ä»£ç åº“ï¼š
1. ä½¿ç”¨[LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory)å®˜æ–¹æŒ‡å¯¼è¿›è¡Œå®‰è£…ã€‚
2. ä½¿ç”¨[DeepSpeed](https://github.com/deepspeedai/DeepSpeed#installation)å®˜æ–¹æŒ‡å¯¼è¿›è¡Œå®‰è£…ï¼ˆå¯é€‰ï¼‰ã€‚
3. å®‰è£…é…å¥—çš„transformeråº“ã€‚å½“å‰æ··å…ƒæäº¤çš„transformerä»£ç æ­£åœ¨è¯„å®¡ä¸­ï¼Œéœ€è¦è·å–é…å¥—çš„åˆ†æ”¯ã€‚
```
pip install git+https://github.com/huggingface/transformers@4970b23cedaf745f963779b4eae68da281e8c6ca
```

### å‡†å¤‡æ•°æ®

æˆ‘ä»¬éœ€è¦å‡†å¤‡è‡ªå®šä¹‰çš„æ•°æ®é›†ï¼š

1. è¯·å°†æ‚¨çš„æ•°æ®ä»¥`json`æ ¼å¼è¿›è¡Œç»„ç»‡ï¼Œå¹¶å°†æ•°æ®æ”¾å…¥`LLaMA-Factory`çš„`data`ç›®å½•ä¸­ã€‚å½“å‰ä½¿ç”¨çš„æ˜¯`sharegpt`æ ¼å¼çš„æ•°æ®é›†ï¼Œéœ€è¦éµå¾ªä»¥ä¸‹æ ¼å¼ï¼š
```
[
  {
    "messages": [
      {
        "role": "system",
        "content": "ç³»ç»Ÿæç¤ºè¯ï¼ˆé€‰å¡«ï¼‰"
      },
      {
        "role": "user",
        "content": "äººç±»æŒ‡ä»¤"
      },
      {
        "role": "assistant",
        "content": "æ¨¡å‹å›ç­”"
      }
    ]
  }
]
```
å¯ä»¥å‚è€ƒå‰é¢ç« èŠ‚ä¸­å¯¹[æ•°æ®æ ¼å¼](#è®­ç»ƒæ•°æ®æ ¼å¼å¤„ç†)çš„è¯´æ˜ã€‚

2. åœ¨`data/dataset_info.json`æ–‡ä»¶ä¸­æä¾›æ‚¨çš„æ•°æ®é›†å®šä¹‰ï¼Œå¹¶é‡‡ç”¨ä»¥ä¸‹æ ¼å¼ï¼š
```
"æ•°æ®é›†åç§°": {
  "file_name": "data.json",
  "formatting": "sharegpt",
  "columns": {
    "messages": "messages"
  },
  "tags": {
    "role_tag": "role",
    "content_tag": "content",
    "user_tag": "user",
    "assistant_tag": "assistant",
    "system_tag": "system"
  }
}
```

### è®­ç»ƒ

1. å°†`llama_factory_support/example_configs`ç›®å½•ä¸‹çš„æ–‡ä»¶éƒ½æ‹·è´åˆ°`LLaMA-Factory`çš„`example/hunyuan`ç›®å½•ä¸‹ã€‚
2. ä¿®æ”¹é…ç½®æ–‡ä»¶`hunyuan_full.yaml`ä¸­çš„æ¨¡å‹è·¯å¾„å’Œæ•°æ®é›†åç§°ï¼Œå…¶ä»–çš„é…ç½®è¯·æ ¹æ®éœ€è¦è¿›è¡Œä¿®æ”¹ã€‚
  ```
  ### model
  model_name_or_path: [!!!add the model path here!!!]

  ### dataset
  dataset: [!!!add the data set name here!!!]
  ```
3. æ‰§è¡Œè®­ç»ƒå‘½ä»¤
    * è¿è¡Œå•æœºè®­ç»ƒ
    è¯·æ³¨æ„è¿™é‡Œéœ€è¦è®¾ç½®`DISABLE_VERSION_CHECK`ç¯å¢ƒå˜é‡ï¼Œé¿å…ç‰ˆæœ¬å†²çªã€‚
    ```
    export DISABLE_VERSION_CHECK=1
    llamafactory-cli train examples/hunyuan/hunyuan_full.yaml
    ```
    * è¿è¡Œå¤šæœºè®­ç»ƒ
    åœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸Šæ‰§è¡Œä»¥ä¸‹å‘½ä»¤ã€‚è¯·æ³¨æ„å°†`torchrun`éœ€è¦çš„`NNODES`ã€`NODE_RANK`ã€`MASTER_ADDR`å’Œ`MASTER_PORT`æŒ‰ç…§æ‚¨è¿è¡Œçš„ç¯å¢ƒè¿›è¡Œé…ç½®ã€‚
    ```
    export DISABLE_VERSION_CHECK=1
    FORCE_TORCHRUN=1 NNODES=${NNODES} NODE_RANK=${NODE_RANK} MASTER_ADDR=${MASTER_ADDR} MASTER_PORT=${MASTER_PORT} \
    llamafactory-cli train examples/hunyuan_full.yaml
    ```

&nbsp;

## æ¨ç†å’Œéƒ¨ç½²

HunyuanLLMå¯ä»¥é‡‡ç”¨TensorRT-LLM, vLLMæˆ–sglangéƒ¨ç½²ã€‚ä¸ºäº†ç®€åŒ–éƒ¨ç½²è¿‡ç¨‹HunyuanLLMæä¾›äº†é¢„æ„å»ºdockeré•œåƒï¼Œè¯¦è§ä¸€ä¸‹ç« èŠ‚ã€‚

é•œåƒï¼šhttps://hub.docker.com/r/hunyuaninfer/hunyuan-7b/tags

## ä½¿ç”¨TensorRT-LLMæ¨ç†
### Docker:

ä¸ºäº†ç®€åŒ–éƒ¨ç½²è¿‡ç¨‹ï¼ŒHunyuanLLMæä¾›äº†é¢„æ„å»ºdockeré•œåƒ (æ³¨æ„ï¼š è¯¥é•œåƒè¦æ±‚Hostçš„Cudaç‰ˆæœ¬ä¸º12.8ä»¥ä¸Šï¼‰ï¼š

[hunyuaninfer/hunyuan-7b:hunyuan-7b-trtllm](https://hub.docker.com/r/hunyuaninfer/hunyuan-7b/tags) ã€‚æ‚¨åªéœ€è¦ä¸‹è½½æ¨¡å‹æ–‡ä»¶å¹¶ç”¨ä¸‹é¢ä»£ç å¯åŠ¨dockerå³å¯å¼€å§‹æ¨ç†æ¨¡å‹ã€‚
```shell
# æ‹‰å–
å›½å†…ï¼š
docker pull docker.cnb.cool/tencent/hunyuan/hunyuan-7b:hunyuan-7b-trtllm
å›½å¤–ï¼š
docker pull hunyuaninfer/hunyuan-7b:hunyuan-7b-trtllm

# å¯åŠ¨
docker run --privileged --user root --name hunyuanLLM_infer --rm -it --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 --gpus=all hunyuaninfer/hunyuan-7b:hunyuan-7b-trtllm
```

æ³¨: Dockerå®¹å™¨æƒé™ç®¡ç†ã€‚ä»¥ä¸Šä»£ç é‡‡ç”¨ç‰¹æƒæ¨¡å¼ï¼ˆ--privilegedï¼‰å¯åŠ¨Dockerå®¹å™¨ä¼šèµ‹äºˆå®¹å™¨è¾ƒé«˜çš„æƒé™ï¼Œå¢åŠ æ•°æ®æ³„éœ²å’Œé›†ç¾¤å®‰å…¨é£é™©ã€‚å»ºè®®åœ¨éå¿…è¦æƒ…å†µä¸‹é¿å…ä½¿ç”¨ç‰¹æƒæ¨¡å¼ï¼Œä»¥é™ä½å®‰å…¨å¨èƒã€‚å¯¹äºå¿…é¡»ä½¿ç”¨ç‰¹æƒæ¨¡å¼çš„åœºæ™¯ï¼Œåº”è¿›è¡Œä¸¥æ ¼çš„å®‰å…¨è¯„ä¼°ï¼Œå¹¶å®æ–½ç›¸åº”çš„å®‰å…¨ç›‘æ§ã€åŠ å›ºæªæ–½ã€‚

### BF16éƒ¨ç½²

#### Step1ï¼šæ‰§è¡Œæ¨ç†

#### æ–¹å¼1ï¼šå‘½ä»¤è¡Œæ¨ç†

ä¸‹é¢æˆ‘ä»¬å±•ç¤ºä¸€ä¸ªä»£ç ç‰‡æ®µï¼Œé‡‡ç”¨`TensorRT-LLM`å¿«é€Ÿè¯·æ±‚chat modelï¼š
ä¿®æ”¹ examples/pytorch/quickstart_advanced.py ä¸­å¦‚ä¸‹ä»£ç ï¼š


```python
def setup_llm(args):
    kv_cache_config = KvCacheConfig(
        enable_block_reuse=not args.disable_kv_cache_reuse,
        free_gpu_memory_fraction=args.kv_cache_fraction,
    )
    spec_config = None

    hf_ckpt_path="$your_hunyuan_model_path"
    tokenizer = AutoTokenizer.from_pretrained(hf_ckpt_path, trust_remote_code=True)
    llm = LLM(
        tokenizer=tokenizer,
        model=args.model_dir,
        backend='pytorch',
        disable_overlap_scheduler=args.disable_overlap_scheduler,
        kv_cache_dtype=args.kv_cache_dtype,
        kv_cache_config=kv_cache_config,
        attn_backend=args.attention_backend,
        use_cuda_graph=args.use_cuda_graph,
        cuda_graph_padding_enabled=args.cuda_graph_padding_enabled,
        cuda_graph_batch_sizes=args.cuda_graph_batch_sizes,
        load_format=args.load_format,
        print_iter_log=args.print_iter_log,
        enable_iter_perf_stats=args.print_iter_log,
        torch_compile_config=TorchCompileConfig(
            enable_fullgraph=args.use_torch_compile,
            enable_inductor=args.use_torch_compile,
            enable_piecewise_cuda_graph= \
                args.use_piecewise_cuda_graph)
        if args.use_torch_compile else None,
        moe_backend=args.moe_backend,
        enable_trtllm_sampler=args.enable_trtllm_sampler,
        max_seq_len=args.max_seq_len,
        max_batch_size=args.max_batch_size,
        max_num_tokens=args.max_num_tokens,
        enable_attention_dp=args.enable_attention_dp,
        tensor_parallel_size=args.tp_size,
        pipeline_parallel_size=args.pp_size,
        moe_expert_parallel_size=args.moe_ep_size,
        moe_tensor_parallel_size=args.moe_tp_size,
        moe_cluster_parallel_size=args.moe_cluster_size,
        enable_chunked_prefill=args.enable_chunked_prefill,
        speculative_config=spec_config,
        trust_remote_code=args.trust_remote_code,
        gather_generation_logits=args.return_generation_logits)

    sampling_params = SamplingParams(
        end_id=127960,
        max_tokens=args.max_tokens,
        temperature=args.temperature,
        top_k=args.top_k,
        top_p=args.top_p,
        return_context_logits=args.return_context_logits,
        return_generation_logits=args.return_generation_logits,
        logprobs=args.logprobs)
    return llm, sampling_params


def main():
    args = parse_arguments()
    prompts = args.prompt if args.prompt else example_prompts

    llm, sampling_params = setup_llm(args)
    new_prompts = []
    for prompt in prompts:
        messages = [{"role": "user", "content": f"{prompt}"}]
        new_prompts.append(
            llm.tokenizer.apply_chat_template(messages,
                                                tokenize=False,
                                                add_generation_prompt=True))
    prompts = new_prompts
    outputs = llm.generate(prompts, sampling_params)

    for i, output in enumerate(outputs):
        prompt = output.prompt
        generated_text = output.outputs[0].text
        print(f"[{i}] Prompt: {prompt!r}, Generated text: {generated_text!r}")
```

è¿è¡Œæ–¹å¼ï¼š

```shell
python3 quickstart_advanced.py --model_dir "HunyuanLLMæ¨¡å‹è·¯å¾„" --tp_size 1
```

#### æ–¹å¼2ï¼šæœåŠ¡åŒ–æ¨ç†

ä¸‹é¢æˆ‘ä»¬å±•ç¤ºä½¿ç”¨`TensorRT-LLM`æœåŠ¡åŒ–çš„æ–¹å¼éƒ¨ç½²æ¨¡å‹å’Œè¯·æ±‚ã€‚

ä»¥tencent/Hunyuan-7B-Instructä¸ºä¾‹
å‡†å¤‡é…ç½®æ–‡ä»¶ï¼š

```
cat >/path/to/extra-llm-api-config.yml <<EOF
use_cuda_graph: true
cuda_graph_padding_enabled: true
cuda_graph_batch_sizes:
- 1
- 2
- 4
- 8
- 16
- 32
print_iter_log: true
EOF
```

å¯åŠ¨æœåŠ¡ï¼š

```shell
trtllm-serve \
  /path/to/HunYuan-7b \
  --host localhost \
  --port 8000 \
  --backend pytorch \
  --max_batch_size 32 \
  --max_num_tokens 16384 \
  --tp_size 1 \
  --kv_cache_free_gpu_memory_fraction 0.6 \
  --trust_remote_code \
  --extra_llm_api_options /path/to/extra-llm-api-config.yml
```

æœåŠ¡å¯åŠ¨æˆåŠŸå, ä½¿ç”¨ OpenAI API è¿›è¡Œæ¨¡å‹æ¨ç†è°ƒç”¨ï¼š
```
curl -X POST "http://localhost:8000/v1/chat/completions" \
  -H "Content-Type: application/json" \
  --data '{
    "model": "HunYuan/HunYuan-7b",
    "messages": [
      {
        "role": "user",
        "content": "Write a short summary of the benefits of regular exercise"
      }
    ]
  }'
```

#### FP8/Int4é‡åŒ–æ¨¡å‹éƒ¨ç½²ï¼š
ç›®å‰ TensorRT-LLM çš„ fp8 å’Œ int4 é‡åŒ–æ¨¡å‹æ­£åœ¨æ”¯æŒä¸­ï¼Œæ•¬è¯·æœŸå¾…ã€‚


## ä½¿ç”¨vLLMæ¨ç†
### ç‰ˆæœ¬è¦æ±‚:

è¯·ä½¿ç”¨vLLM v0.10.0ä¹‹åçš„ç‰ˆæœ¬è¿›è¡Œéƒ¨ç½²å’Œæ¨ç†

éœ€è¦å®‰è£…æŒ‡å®šç‰ˆæœ¬çš„transformersï¼Œæˆ‘ä»¬å°†åœ¨ä¸ä¹…åå®Œæˆå¯¹transformersä¸»åˆ†æ”¯çš„åˆå…¥
```SHELL
pip install git+https://github.com/huggingface/transformers@4970b23cedaf745f963779b4eae68da281e8c6ca
```
### BF16éƒ¨ç½²

ä»¥tencent/Hunyuan-7B-Instructä¸ºä¾‹ï¼Œå·²ç»é€šè¿‡ä¸Šè¿°çš„transformersè·å–äº†æ¨¡å‹åœ°å€
```shell
export MODEL_PATH=PATH_TO_MODEL
```

#### Step1ï¼šæ‰§è¡Œæ¨ç†

#### æ–¹å¼1ï¼šå‘½ä»¤è¡Œæ¨ç†

ä¸‹é¢æˆ‘ä»¬å±•ç¤ºä¸€ä¸ªä»£ç ç‰‡æ®µï¼Œé‡‡ç”¨`vLLM`å¿«é€Ÿè¯·æ±‚chat modelï¼š

æ³¨: vLLMç»„ä»¶è¿œç¨‹ä»£ç æ‰§è¡Œé˜²æŠ¤ã€‚ä¸‹åˆ—ä»£ç ä¸­vLLMç»„ä»¶çš„trust-remote-codeé…ç½®é¡¹è‹¥è¢«å¯ç”¨ï¼Œå°†å…è®¸åŠ è½½å¹¶æ‰§è¡Œæ¥è‡ªè¿œç¨‹æ¨¡å‹ä»“åº“çš„ä»£ç ï¼Œè¿™å¯èƒ½å¯¼è‡´æ¶æ„ä»£ç çš„æ‰§è¡Œã€‚é™¤éä¸šåŠ¡éœ€æ±‚æ˜ç¡®è¦æ±‚ï¼Œå¦åˆ™å»ºè®®è¯¥é…ç½®é¡¹å¤„äºç¦ç”¨çŠ¶æ€ï¼Œä»¥é™ä½æ½œåœ¨çš„å®‰å…¨å¨èƒã€‚


```python
import os
from typing import List, Optional
from vllm import LLM, SamplingParams
from vllm.inputs import PromptType
from transformers import AutoTokenizer

model_path=os.environ.get('MODEL_PATH')
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)

llm = LLM(model=model_path,
        tokenizer=model_path,
        trust_remote_code=True,
        dtype='bfloat16',
        tensor_parallel_size=4,
        gpu_memory_utilization=0.9)

sampling_params = SamplingParams(
    temperature=0.7, top_p=0.8, max_tokens=4096, top_k=20, repetition_penalty=1.05)

messages = [
    {
        "role": "system",
        "content": "You are a helpful assistant.",
    },
    {"role": "user", "content": "Write a short summary of the benefits of regular exercise"},
]

tokenized_chat = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors="pt")

dummy_inputs: List[PromptType] = [{
    "prompt_token_ids": batch
} for batch in tokenized_chat.numpy().tolist()]

outputs = llm.generate(dummy_inputs, sampling_params)

# Print the outputs.
for output in outputs:
    prompt = output.prompt
    generated_text = output.outputs[0].text
    print(f"Prompt: {prompt!r}, Generated text: {generated_text!r}")
```

#### æ–¹å¼2ï¼šæœåŠ¡åŒ–æ¨ç†

ä¸‹é¢æˆ‘ä»¬å±•ç¤ºä½¿ç”¨`vLLM`æœåŠ¡åŒ–çš„æ–¹å¼éƒ¨ç½²æ¨¡å‹å¹¶è¯·æ±‚

æˆ‘ä»¬å¯åŠ¨æœåŠ¡ï¼Œè¿è¡Œ :
```shell
python3 -m vllm.entrypoints.openai.api_server \
    --host 0.0.0.0 \
    --port 8000 \
    --trust-remote-code \
    --model ${MODEL_PATH} \
    --tensor-parallel-size 1 \
    --dtype bfloat16 \
    --quantization experts_int8 \
    --served-model-name hunyuan \
    2>&1 | tee log_server.txt
```

è¿è¡ŒæˆåŠŸå, è¿è¡Œè¯·æ±‚è„šæœ¬ï¼š
```shell
curl http://0.0.0.0:8000/v1/chat/completions -H 'Content-Type: application/json' -d '{
"model": "hunyuan",
"messages": [
    {
        "role": "system",
        "content": [{"type": "text", "text": "You are a helpful assistant."}]
    },
    {
        "role": "user",
        "content": [{"type": "text", "text": "è¯·æŒ‰é¢ç§¯å¤§å°å¯¹å››å¤§æ´‹è¿›è¡Œæ’åºï¼Œå¹¶ç»™å‡ºé¢ç§¯æœ€å°çš„æ´‹æ˜¯å“ªä¸€ä¸ªï¼Ÿç›´æ¥è¾“å‡ºç»“æœã€‚"}]
    }
],
"max_tokens": 2048,
"temperature":0.7,
"top_p": 0.6,
"top_k": 20,
"repetition_penalty": 1.05,
"stop_token_ids": [127960]
}'
```

### é‡åŒ–æ¨¡å‹éƒ¨ç½²ï¼š

æœ¬éƒ¨åˆ†ä»‹ç»é‡‡ç”¨vLLMéƒ¨ç½²é‡åŒ–åæ¨¡å‹çš„æµç¨‹ã€‚


#### Int8é‡åŒ–æ¨¡å‹éƒ¨ç½²ï¼š
éƒ¨ç½²Int8-weight-onlyç‰ˆæœ¬HunYuan-7Bæ¨¡å‹

æˆ‘ä»¬å¯åŠ¨Int8æœåŠ¡ï¼Œè¿è¡Œï¼š
```shell
python3 -m vllm.entrypoints.openai.api_server \
    --host 0.0.0.0 \
    --port 8000 \
    --trust-remote-code \
    --model ${MODEL_PATH} \
    --tensor-parallel-size 1 \
    --dtype bfloat16 \
    --served-model-name hunyuan \
    --quantization experts_int8 \
    2>&1 | tee log_server.txt
```

#### Int4é‡åŒ–æ¨¡å‹éƒ¨ç½²ï¼š
éƒ¨ç½²Int4-weight-onlyç‰ˆæœ¬HunYuan-7Bæ¨¡å‹ï¼Œé‡‡ç”¨GPTQæ–¹å¼ï¼š

```shell
export MODEL_PATH=PATH_TO_INT4_MODEL
```
æ¥ç€æˆ‘ä»¬å¯åŠ¨Int4æœåŠ¡ï¼Œè¿è¡Œï¼š
```shell
python3 -m vllm.entrypoints.openai.api_server \
    --host 0.0.0.0 \
    --port 8000 \
    --trust-remote-code \
    --model ${MODEL_PATH} \
    --tensor-parallel-size 1 \
    --dtype bfloat16 \
    --served-model-name hunyuan \
    --quantization gptq_marlin \
    2>&1 | tee log_server.txt
```


#### FP8é‡åŒ–æ¨¡å‹éƒ¨ç½²ï¼š
éƒ¨ç½²W8A8C8ç‰ˆæœ¬HunYuan-7Bæ¨¡å‹

æˆ‘ä»¬å¯åŠ¨FP8æœåŠ¡ï¼Œè¿è¡Œï¼š
```shell
python3 -m vllm.entrypoints.openai.api_server \
    --host 0.0.0.0 \
    --port 8000 \
    --trust-remote-code \
    --model ${MODEL_PATH} \
    --tensor-parallel-size 1 \
    --dtype bfloat16 \
    --served-model-name hunyuan \
    --kv-cache-dtype fp8 \
    2>&1 | tee log_server.txt
```

## ä½¿ç”¨sglangæ¨ç†

### BF16éƒ¨ç½²

#### Step1: æ‹‰å–é•œåƒ


```
docker pull lmsysorg/sglang:latest
```

- å¯åŠ¨ API server:

```
docker run --entrypoint="python3" --gpus all \
    --shm-size 32g \
    -p 30000:30000 \
    --ulimit nproc=10000 \
    --privileged \
    --ipc=host \
     lmsysorg/sglang:latest \
    -m sglang.launch_server --model-path hunyuan/huanyuan_7B --tp 1 --trust-remote-code --host 0.0.0.0 --port 30000
```

#### Step2ï¼šæ‰§è¡Œæ¨ç†

#### æ–¹å¼1ï¼šå‘½ä»¤è¡Œæ¨ç†

ä¸‹é¢æˆ‘ä»¬å±•ç¤ºä¸€ä¸ªä»£ç ç‰‡æ®µï¼Œé‡‡ç”¨`sglang`å¿«é€Ÿè¯·æ±‚chat modelï¼š


```python
import sglang as sgl
from transformers import AutoTokenizer

model_path=os.environ.get('MODEL_PATH')


tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)

messages = [
    {
        "role": "system",
        "content": "You are a helpful assistant.",
    },
    {"role": "user", "content": "Write a short summary of the benefits of regular exercise"},
]
prompts = []
prompts.append(tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
))
print(prompts)

llm = sgl.Engine(
    model_path=model_path,
    tp_size=1,
    trust_remote_code=True,
    mem_fraction_static=0.7,
)

sampling_params = {"temperature": 0.7, "top_p": 0.8, "top_k": 20, "max_new_tokens": 4096}
outputs = llm.generate(prompts, sampling_params)
for prompt, output in zip(prompts, outputs):
    print(f"Prompt: {prompt}\nGenerated text: {output['text']}")
```

#### æ–¹å¼2ï¼šæœåŠ¡åŒ–æ¨ç†

ä¸‹é¢æˆ‘ä»¬å±•ç¤ºä½¿ç”¨`sglang`æœåŠ¡åŒ–çš„æ–¹å¼éƒ¨ç½²æ¨¡å‹å’Œè¯·æ±‚ã€‚

```shell
model_path="HunyuanLLMæ¨¡å‹è·¯å¾„"
python3 -u -m sglang.launch_server \
    --model-path $model_path \
    --tp 4 \
    --trust-remote-code
```

æœåŠ¡å¯åŠ¨æˆåŠŸå, è¿è¡Œè¯·æ±‚è„šæœ¬ï¼š
```python
import openai
client = openai.Client(
    base_url="http://localhost:30000/v1", api_key="EMPTY")

response = client.chat.completions.create(
    model="default",
    messages= [
        {"role": "user", "content": "Write a short summary of the benefits of regular exercise"},
    ],
    temperature=0.7,
    max_tokens=4096,
    extra_body={"top_p": 0.8, "top_k": 20}
)
print(response)
```

Citing Hunyuan-MT:

```bibtex
@misc{hunyuan_mt,
      title={Hunyuan-MT Technical Report}, 
      author={Mao Zheng and Zheng Li and Bingxin Qu and Mingyang Song and Yang Du and Mingrui Sun and Di Wang},
      year={2025},
      eprint={2509.05209},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2509.05209}, 
}
```

## è”ç³»æˆ‘ä»¬
å¦‚æœä½ æƒ³ç»™æˆ‘ä»¬çš„ç ”å‘å’Œäº§å“å›¢é˜Ÿç•™è¨€ï¼Œæ¬¢è¿è”ç³»æˆ‘ä»¬è…¾è®¯æ··å…ƒLLMå›¢é˜Ÿã€‚ä½ å¯ä»¥é€šè¿‡é‚®ä»¶ï¼ˆhunyuan_opensource@tencent.comï¼‰è”ç³»æˆ‘ä»¬ã€‚
