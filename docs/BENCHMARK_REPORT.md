# HY-MT 多模型性能测试报告

**测试时间**: 2026-01-03 18:55:54  
**测试环境**: Docker 容器 (neosun/hy-mt:v1.2.0)  
**GPU**: NVIDIA GPU (45GB VRAM)

## 测试说明

### 测试方法
- 每个模型在每种文本类型下测试 3 次
- 取最快一次作为最终结果（排除预热影响）
- 翻译方向：英文 → 中文

### 测试文本

| 类型 | 字符数 | 说明 |
|------|--------|------|
| 短文本 | 61 | 单句 |
| 中等文本 | 530 | 一段话 |
| 长文本 | 1,858 | 多段落 |
| 超长文本 | 4,213 | 长篇文章（需分块处理） |

## 测试结果

### 性能对比表（最佳耗时 ms）

| 模型 | 短文本 | 中等文本 | 长文本 | 超长文本 |
|------|--------|----------|--------|----------|
| **HY-MT 1.8B** | 418 | 3,626 | 14,016 | 32,294 |
| HY-MT 1.8B FP8 | 1,146 | 10,767 | 38,092 | 92,906 |
| **HY-MT 7B** | 415 | 4,355 | 17,708 | 42,990 |
| HY-MT 7B FP8 | 2,897 | 28,477 | 115,601 | 274,137 |

### 性能对比表（秒）

| 模型 | 短文本 | 中等文本 | 长文本 | 超长文本 |
|------|--------|----------|--------|----------|
| **HY-MT 1.8B** | 0.4s | 3.6s | 14.0s | 32.3s |
| HY-MT 1.8B FP8 | 1.1s | 10.8s | 38.1s | 92.9s |
| **HY-MT 7B** | 0.4s | 4.4s | 17.7s | 43.0s |
| HY-MT 7B FP8 | 2.9s | 28.5s | 115.6s | 274.1s |

## 详细测试数据

### HY-MT 1.8B (基础模型)
- 显存需求: 6GB
- 推荐场景: 日常翻译，平衡速度与质量

| 文本类型 | 第1次 | 第2次 | 第3次 | 最佳 |
|----------|-------|-------|-------|------|
| 短文本 | 422ms | 422ms | 418ms | **418ms** |
| 中等文本 | 3,626ms | 3,746ms | 3,697ms | **3,626ms** |
| 长文本 | 14,016ms | 14,390ms | 14,309ms | **14,016ms** |
| 超长文本 | 33,422ms | 32,305ms | 32,294ms | **32,294ms** |

### HY-MT 1.8B FP8 (量化版)
- 显存需求: 4GB
- 推荐场景: 显存受限环境

| 文本类型 | 第1次 | 第2次 | 第3次 | 最佳 |
|----------|-------|-------|-------|------|
| 短文本 | 1,189ms | 1,146ms | 1,149ms | **1,146ms** |
| 中等文本 | 10,767ms | 11,164ms | 11,237ms | **10,767ms** |
| 长文本 | 39,604ms | 39,342ms | 38,092ms | **38,092ms** |
| 超长文本 | 92,906ms | 93,811ms | 95,215ms | **92,906ms** |

### HY-MT 7B (大模型)
- 显存需求: 16GB
- 推荐场景: 高质量翻译需求

| 文本类型 | 第1次 | 第2次 | 第3次 | 最佳 |
|----------|-------|-------|-------|------|
| 短文本 | 462ms | 458ms | 415ms | **415ms** |
| 中等文本 | 4,355ms | 4,411ms | 4,359ms | **4,355ms** |
| 长文本 | 17,708ms | 18,157ms | 18,027ms | **17,708ms** |
| 超长文本 | 42,990ms | 43,216ms | 43,010ms | **42,990ms** |

### HY-MT 7B FP8 (大模型量化版)
- 显存需求: 10GB
- 推荐场景: 需要 7B 质量但显存有限

| 文本类型 | 第1次 | 第2次 | 第3次 | 最佳 |
|----------|-------|-------|-------|------|
| 短文本 | 2,897ms | 3,018ms | 3,049ms | **2,897ms** |
| 中等文本 | 28,528ms | 28,842ms | 28,477ms | **28,477ms** |
| 长文本 | 117,408ms | 116,479ms | 115,601ms | **115,601ms** |
| 超长文本 | 274,137ms | 274,346ms | 274,465ms | **274,137ms** |

## 分析与结论

### 速度排名（从快到慢）

1. **HY-MT 1.8B** - 最快，适合实时翻译
2. **HY-MT 7B** - 略慢于 1.8B，但质量更高
3. **HY-MT 1.8B FP8** - 比基础版慢约 2.7 倍
4. **HY-MT 7B FP8** - 最慢，比 7B 基础版慢约 6.4 倍

### 🔍 为什么 FP8 量化模型反而更慢？

这是一个**反直觉但符合技术原理**的现象：

#### 1. FP8 量化的真正目的

| 目标 | 是否达成 | 说明 |
|------|----------|------|
| 减少显存占用 | ✅ 是 | 模型大小减少约 50% |
| 加速推理 | ❌ 否 | 这不是 FP8 的设计目标 |

**FP8 量化的核心价值是让大模型能在显存受限的 GPU 上运行，而不是加速。**

#### 2. 为什么会变慢？

```
存储格式: FP8 (8-bit) → 推理计算: BF16/FP16 (16-bit)
                ↑
         需要动态解压缩
```

- **运行时解压缩开销**: FP8 模型存储为 8-bit 压缩格式，但 GPU 计算时需要解压缩回 16-bit
- **每次前向传播都要解压**: 这个开销在每个 token 生成时都会发生
- **软件实现 vs 硬件加速**: `compressed-tensors` 库使用通用实现，未针对特定硬件优化

#### 3. 硬件因素

| GPU 架构 | FP8 原生支持 | 说明 |
|----------|-------------|------|
| H100/H200 | ✅ 完全支持 | 专为 FP8 设计的 Tensor Core |
| L40S (本测试) | ⚠️ 部分支持 | Ada 架构支持 FP8，但库未充分利用 |
| A100 | ❌ 不支持 | 需要软件模拟 |
| RTX 3090 | ❌ 不支持 | 需要软件模拟 |

本测试环境使用 **NVIDIA L40S** (Ada Lovelace, compute 8.9)，理论上支持 FP8 硬件加速，
但 `compressed-tensors` 库的实现可能未充分利用硬件特性。

#### 4. 速度下降倍数分析

| 模型对比 | 速度下降 | 原因 |
|----------|----------|------|
| 1.8B FP8 vs 1.8B | 2.7x 慢 | 解压缩开销相对较小 |
| 7B FP8 vs 7B | 6.4x 慢 | 更大模型 = 更多解压缩操作 |

7B 模型的 FP8 版本下降更多，因为：
- 参数量是 1.8B 的 ~4 倍
- 解压缩操作量也相应增加
- 内存带宽成为瓶颈

### 关键发现

1. **FP8 量化的代价**
   - 1.8B FP8 比基础版慢 **2.7-2.9 倍**
   - 7B FP8 比基础版慢 **6.4-7.0 倍**
   - FP8 量化虽然节省显存，但推理速度显著下降
   - **这是预期行为，不是 bug**

2. **1.8B vs 7B 基础模型**
   - 短文本：速度几乎相同
   - 长文本：7B 比 1.8B 慢约 **26-33%**
   - 7B 模型在保持较高速度的同时提供更好的翻译质量
   - **7B 是性价比最高的选择**（如果显存足够）

3. **文本长度与耗时关系**
   - 耗时与文本长度大致呈线性关系
   - 超长文本会触发分块处理机制

### 💡 什么时候应该用 FP8？

| 场景 | 是否推荐 FP8 | 理由 |
|------|-------------|------|
| 显存充足 (≥16GB) | ❌ 不推荐 | 用基础模型更快 |
| 显存受限 (6-10GB) | ⚠️ 可考虑 | 牺牲速度换取可用性 |
| 显存极限 (<6GB) | ✅ 推荐 | 唯一能运行的选择 |
| 批量处理 | ❌ 不推荐 | 速度损失会累积 |
| 实时翻译 | ❌ 不推荐 | 延迟太高 |

### 🏆 最佳模型选择

**如果显存足够 (≥16GB)，HY-MT 7B 是最佳选择：**
- 翻译质量最高
- 速度仅比 1.8B 慢 20-30%
- 无量化损失

### 推荐选择

| 场景 | 推荐模型 | 理由 |
|------|----------|------|
| 实时翻译/API 服务 | HY-MT 1.8B | 速度最快 |
| 高质量翻译 | HY-MT 7B | 质量最佳，速度可接受 |
| 显存 < 6GB | HY-MT 1.8B FP8 | 唯一选择 |
| 显存 6-16GB | HY-MT 1.8B 或 7B | 根据质量需求选择 |

## 测试文本样例

### 短文本 (61 字符)
```
Artificial intelligence is transforming how we live and work.
```

### 中等文本 (530 字符)
```
Machine learning has revolutionized many industries in recent years. 
From healthcare to finance, AI systems are now capable of analyzing vast amounts of data 
and making predictions that were previously impossible...
```

### 长文本 (1,858 字符)
```
The rapid advancement of artificial intelligence and machine learning technologies 
has fundamentally transformed the landscape of modern computing...
```

### 超长文本 (4,213 字符)
```
The history of artificial intelligence spans several decades, beginning with 
the foundational work of pioneers like Alan Turing...
```

---

*报告生成时间: 2026-01-03*
